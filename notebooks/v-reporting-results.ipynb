{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa81e98c",
   "metadata": {},
   "source": [
    "# An In-depth Evaluation of Approaches to Text Classification (IDEATC)\n",
    "\n",
    "## V. Reporting Results\n",
    "\n",
    "_This notebook is to create output tables and figures for the paper._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d85b7b45",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# data wrangling\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# machine learning\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import classification_report, top_k_accuracy_score\n",
    "\n",
    "# local packages\n",
    "import src\n",
    "\n",
    "# other settings\n",
    "LOAD_PATH_DATASET = Path(os.pardir, 'data', 'processed')\n",
    "LOAD_PATH_PROMPTING = Path(os.pardir, 'data', 'prompting')\n",
    "SAVE_PATH_RESULTS = Path(os.pardir, 'data', 'results')\n",
    "SAVE_PATH_FIGURES = Path(os.pardir, 'figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I. Overall Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_table = src.experiments.utils.show_best_results(SAVE_PATH_RESULTS)\n",
    "df_table = df_table.join(df_table.mean(axis=1).rename('macroaverage')).round(2)\n",
    "df_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series_supervised = df_table.loc['deberta_v3_small_finetuned']\n",
    "series_zeroshot = df_table.loc[['deberta_v3_small_zeroshot', 'deberta_v3_xsmall_zeroshot', 'deberta_v3_base_zeroshot']].max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series_zeroshot.divide(series_supervised).to_frame().T.multiply(100).round(1)#.to_clipboard(index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_table.to_clipboard(index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. Learning Curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name2path = {\n",
    "    'Rotten Tomatoes': Path('../data/processed/rotten_tomatoes_processed'),\n",
    "    'IMDb': Path('../data/processed/imdb_processed'),\n",
    "    'Yelp-2': Path('../data/processed/yelp_polarity_processed'),\n",
    "    'Yelp-5': Path('../data/processed/yelp_review_full_processed'),\n",
    "    'SST-5': Path('../data/processed/setfit_sst5_processed'),\n",
    "    'Dynasent (R2)': Path('../data/processed/dynabench_dynasent_processed'),\n",
    "    'AG News': Path('../data/processed/ag_news_processed'),\n",
    "    '20 Newsgroups': Path('../data/processed/20_newsgroups_processed'),\n",
    "    'DBpedia14': Path('../data/processed/dbpedia_14_processed'),\n",
    "    'Web of Science': Path('../data/processed/web_of_science_processed'),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_df_metrics = [src.experiments.utils.read_metrics(SAVE_PATH_RESULTS, path.name) for path in name2path.values()]\n",
    "fig = src.plotting.plot_performance_overall_all(list_df_metrics, list(name2path))\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1200,\n",
    ")\n",
    "fig.write_image(SAVE_PATH_FIGURES.joinpath('figure_1.svg'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_names = ('yelp_review_full_processed', 'setfit_sst5_processed')\n",
    "for idx, dataset_name in enumerate(dataset_names, start=1):\n",
    "    df_metrics = src.experiments.utils.read_metrics(SAVE_PATH_RESULTS, dataset_name)\n",
    "    fig = src.plotting.plot_performance_by_class(df_metrics=df_metrics, per_row=5, target_order=['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive'])\n",
    "    fig.update_layout(height=360, width=1200)\n",
    "    fig.write_image(SAVE_PATH_FIGURES.joinpath(f'figure_2_{idx}.svg'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_names = ('ag_news_processed', 'dbpedia_14_processed')\n",
    "for idx, dataset_name in enumerate(dataset_names, start=1):\n",
    "    df_metrics = src.experiments.utils.read_metrics(SAVE_PATH_RESULTS, dataset_name)\n",
    "    fig = src.plotting.plot_performance_by_class(df_metrics=df_metrics, per_row=4 if idx == 1 else 5)\n",
    "    fig.update_layout(height=360 if idx == 1 else 600, width=1200)\n",
    "    fig.write_image(SAVE_PATH_FIGURES.joinpath(f'figure_3_{idx}.svg'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III. Top k Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = '20_newsgroups_processed'\n",
    "scores = xr.open_dataarray(f'../data/results/{dataset_name}_deberta_v3_base_zeroshot_logits.nc')\n",
    "dataset = datasets.load_from_disk(LOAD_PATH_DATASET.joinpath(dataset_name))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for multilabel classification, use softmax over entailment and contraduction within each example, the second column gives the desired probabilities\n",
    "probs = softmax(scores.loc[:, :, ['contradiction', 'entailment']], axis=2)\n",
    "probs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for multiclass classification, use softmax over entailment\n",
    "probs = softmax(scores.sel(classes='entailment'), axis=1)\n",
    "probs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(y_true=dataset['test']['label'], y_pred=probs.argmax(axis=1)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_names = {\n",
    "    'ag_news_processed': 'AG News',\n",
    "    '20_newsgroups_processed': '20 Newsgroups',\n",
    "    'dbpedia_14_processed': 'DBpedia14',\n",
    "    'web_of_science_processed': 'Web of Science',\n",
    "}\n",
    "records = list()\n",
    "for dataset_name in dataset_names:\n",
    "    dataset = datasets.load_from_disk(LOAD_PATH_DATASET.joinpath(dataset_name))\n",
    "    for path in SAVE_PATH_RESULTS.glob(f'{dataset_name}*.nc'):\n",
    "        experiment_id = path.name.split('processed_')[-1].split('_logits')[0]\n",
    "        scores = xr.open_dataarray(path)\n",
    "        probs = softmax(scores.sel(classes='entailment'), axis=1)\n",
    "        for k in range(1, 6):\n",
    "            accuracy = top_k_accuracy_score(y_true=dataset['test']['label'], y_score=probs, k=k)\n",
    "            record = {\n",
    "                'experiment_id': experiment_id,\n",
    "                'dataset': dataset_name,\n",
    "                'k': k,\n",
    "                'top_k_accuracy': accuracy,\n",
    "            }\n",
    "            records.append(record)\n",
    "df_accuracy = pd.DataFrame(records); del records\n",
    "df_accuracy['experiment_id'].replace(src.plotting.get_name_map(), inplace=True)\n",
    "df_accuracy['dataset'].replace(dataset_names, inplace=True)\n",
    "print('Shape:', df_accuracy.shape)\n",
    "display(df_accuracy.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = src.plotting.plot_top_k_accuracy(df_accuracy)\n",
    "fig.update_layout(\n",
    "    height=360,\n",
    "    width=1200,\n",
    ")\n",
    "fig.write_image(SAVE_PATH_FIGURES.joinpath(f'figure_4.svg'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IV. Prompting Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'This example is {}.',\n",
    "    '{}',\n",
    "    'This example expresses a {} sentiment.',\n",
    "    'This example expresses a {} feeling.',\n",
    "    'This example expresses a {} attitude.',\n",
    "    'This example expresses a {} opinion.',\n",
    "]\n",
    "\n",
    "df_prompts = pd.concat([pd.read_csv(path) for path in sorted(LOAD_PATH_PROMPTING.glob('*dynabench_dynasent_processed*metrics.csv'))])\n",
    "df_prompts['experiment_id'] = df_prompts['experiment_id'].str.split('_prompt_')\n",
    "df_prompts['prompt'] = df_prompts['experiment_id'].str.get(1).astype(int).replace(dict(enumerate(prompts)))\n",
    "df_prompts['experiment_id'] = df_prompts['experiment_id'].str.get(0)\n",
    "df_prompts['experiment_id'].replace(src.plotting.get_name_map(), inplace=True)\n",
    "df_prompts = df_prompts.query('target == \"total_weighted\"')[['experiment_id', 'prompt', 'precision', 'recall', 'fscore']].copy()\n",
    "print('Shape:', df_prompts.shape)\n",
    "display(df_prompts.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = src.plotting.plot_prompt_performance(df_prompts)\n",
    "fig.write_image(SAVE_PATH_FIGURES.joinpath(f'figure_5_1.svg'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'This example is {}.',\n",
    "    '{}',\n",
    "    'This example is about {}.',\n",
    "    'This main topic of this text is {}.',\n",
    "    'This example is World News, Sports...',\n",
    "]\n",
    "\n",
    "df_prompts = pd.concat([pd.read_csv(path) for path in sorted(LOAD_PATH_PROMPTING.glob('*ag_news_processed*metrics.csv'))])\n",
    "df_prompts['experiment_id'] = df_prompts['experiment_id'].str.split('_prompt_')\n",
    "df_prompts['prompt'] = df_prompts['experiment_id'].str.get(1).astype(int).replace(dict(enumerate(prompts)))\n",
    "df_prompts['experiment_id'] = df_prompts['experiment_id'].str.get(0)\n",
    "df_prompts['experiment_id'].replace(src.plotting.get_name_map(), inplace=True)\n",
    "df_prompts = df_prompts.query('target == \"total_weighted\"')[['experiment_id', 'prompt', 'precision', 'recall', 'fscore']].copy()\n",
    "print('Shape:', df_prompts.shape)\n",
    "display(df_prompts.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = src.plotting.plot_prompt_performance(df_prompts)\n",
    "fig.write_image(SAVE_PATH_FIGURES.joinpath(f'figure_5_2.svg'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "co-working",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
